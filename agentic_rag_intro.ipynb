{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7c7b37-b2e6-4aa6-afed-fb6ac4247d38",
   "metadata": {},
   "source": [
    "# Agentic RAG - Retrieval Augmented Generation use case for ðŸŽ© Alfred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd868e-5c84-4258-b4c6-c2d5abdc5a11",
   "metadata": {},
   "source": [
    "## Very brief RAG recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f37516-df73-456f-b507-d1356c848865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/non_rag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04565e4-3ca4-48de-badd-6c53cf72f1b2",
   "metadata": {},
   "source": [
    "In a non-RAG LLM application, the user sends a query to an LLM and receives a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd2aea-ed86-4de1-a6b2-084c67338a73",
   "metadata": {},
   "source": [
    "In RAG we leverage the capabilities of the LLM while attempting to give it **additional, relevant information** for our use case, and to **prevent it from giving responses outside of this context**.\n",
    "\n",
    "#### Additional Data\n",
    "We have some data sources, which we store in a database.\n",
    "A very common way to process this data is:\n",
    "1. **chunking** -> larger texts are cut into some small subsets\n",
    "2. **embedding** -> these chunks are not kept as plain text but rather embedded as semantic vectors\n",
    "3. **vector database** -> an efficient way of storing embeddings (e.g. FAISS, Weaviate, etc)\n",
    "\n",
    "The design decisions on chunk size, embedding model etc can affect the performance of the RAG application, and are use-case dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06eceb5-4fac-4fcd-b1a2-e29ef5a5fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/data_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6485c83-7576-45e6-9de3-34b673b49f5c",
   "metadata": {},
   "source": [
    "#### Retrieval: The âœ¨Râœ¨ in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbceac-21ab-4659-bcad-5a950ef8a090",
   "metadata": {},
   "source": [
    "In order to be able to use the information from the knowledge database, we need to search for it. \n",
    "In the Retrieval step, our application:\n",
    "1. Takes the user query\n",
    "2. Passes it through the same embedding model as the data\n",
    "3. Checks the vector database for the most semantically similar vectors of the result\n",
    "4. Returns those chunks of data (if present - if not, returns nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df8411-ad48-4c59-9fc8-ab9581db8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/retrieval_step.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2661f-fe2d-44af-8918-598a104d9d07",
   "metadata": {},
   "source": [
    "#### Augmentation: the âœ¨Aâœ¨ in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60546cbf-43cc-4392-88d7-e025bbfe05d6",
   "metadata": {},
   "source": [
    "Now we have:\n",
    "- our original query\n",
    "- some semantically relevant additional info from our data\n",
    "\n",
    "The next step is to combine them into a prompt for the LLM, which will give us **more specific, less irrelevant** responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073e019-c6ff-4a25-8469-6b94dda722c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/augmentation_step.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7256a-1c7c-406b-b3c4-d4a82c87758d",
   "metadata": {},
   "source": [
    "#### Generation: the âœ¨Gâœ¨ in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c5ee1-3407-4aa7-8e8b-7300cf450537",
   "metadata": {},
   "source": [
    "Now our LLM has a prompt sent to it which contains instructions to **only** give an answer if there are semantically relevant chunks present for our query. \n",
    "\n",
    "It can generate a response based on this, and this response can also be \"I don't know\" or \"I do not have any information on this\" or similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120925c5-4c7e-459b-af4f-03df29649b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/generation_step.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d171b0c-cf60-443e-a0b2-f86d2e948c87",
   "metadata": {},
   "source": [
    "#### Whole typical RAG flow summarised:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ac8d7-0e0b-48fa-ad10-3d9113b6e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/whole_rag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6d461-0ea9-4094-a877-4f54a36839b3",
   "metadata": {},
   "source": [
    "<font size=\"0.5\">  \n",
    "    Diagrams made using images from <a href=\"https://thenounproject.com\">The Noun Project</a> \n",
    "    <ul>\n",
    "     <li>computer user by Luis Prado from <a href=\"https://thenounproject.com/browse/icons/term/computer-user/\" target=\"_blank\" title=\"computer user Icons\">Noun Project</a> (CC BY 3.0)</li>  \n",
    "     <li>Chatbot by Gonza Monta from <a href=\"https://thenounproject.com/browse/icons/term/chatbot/\" target=\"_blank\" title=\"Chatbot Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>brain circuit by Gonza Monta from <a href=\"https://thenounproject.com/browse/icons/term/brain-circuit/\" target=\"_blank\" title=\"brain circuit Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>Files by Design Circle from <a href=\"https://thenounproject.com/browse/icons/term/files/\" target=\"_blank\" title=\"Files Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>API by SAM Designs from <a href=\"https://thenounproject.com/browse/icons/term/api/\" target=\"_blank\" title=\"API Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>matrix by Agan24 from <a href=\"https://thenounproject.com/browse/icons/term/matrix/\" target=\"_blank\" title=\"matrix Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>chocolate chunks by ProSymbols from <a href=\"https://thenounproject.com/browse/icons/term/chocolate-chunks/\" target=\"_blank\" title=\"chocolate chunks Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "     <li>database by Icon Designer from <a href=\"https://thenounproject.com/browse/icons/term/database/\" target=\"_blank\" title=\"database Icons\">Noun Project</a> (CC BY 3.0)</li>\n",
    "    </ul>\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec100a-09e2-4c4e-a0c0-e3580d4e0367",
   "metadata": {},
   "source": [
    "#### In Alfred's case - BM25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d228e-30bc-49a5-8835-56b856c77b5b",
   "metadata": {},
   "source": [
    "NB. The suggested retriever tool in the tutorial does **not** use embeddings, and rather starts off with a tool called BM25 (Best Match 25)\n",
    "- [here](https://watercrawl.dev/blog/Building-on-RAG) is a blog post explaining how both work and compating them\n",
    "- it uses a scoring based on IDF, and term frequencies\n",
    "- it is good for keyword searching, when e.g. exact product names or person names are relevant\n",
    "- it ranks documents based on the score and returns the top matches\n",
    "- see also [here](https://medium.com/@yashwanths_29644/retrieval-augmented-generation-rag-06-bm25-retriever-when-and-why-to-use-it-with-code-demo-132ed70c6bfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2720ba-815b-4b00-ac97-016dfa9ec821",
   "metadata": {},
   "source": [
    "## Basic Tutorial workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c3313-a459-4642-b727-ce352d120363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv, dotenv_values, find_dotenv\n",
    "import os\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.tools import Tool\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())# reads variables from a .env file and sets them in os.environ\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c77d1-997a-4ac2-adb9-b271aafefdb5",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed5838-061b-475a-a041-a079f479c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join([\n",
    "            f\"Name: {guest['name']}\",\n",
    "            f\"Relation: {guest['relation']}\",\n",
    "            f\"Description: {guest['description']}\",\n",
    "            f\"Email: {guest['email']}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest[\"name\"]}\n",
    "    )\n",
    "    for guest in guest_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a838a-41b8-43e8-9999-5aa216e65234",
   "metadata": {},
   "source": [
    "### Creating Retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566469c7-ee21-490f-8813-80c83ff186dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.invoke(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf74db-b846-4e30-9196-19f678791c1e",
   "metadata": {},
   "source": [
    "### Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ab837-7729-440f-9676-f680a83c08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-Coder-Next\", #model in materials didn't work, newer one does\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c4e7c-952d-488c-b515-b79ee4d079dd",
   "metadata": {},
   "source": [
    "#### Experiment 1: retrieving information about a specific guest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917af65-0cd1-454c-96c2-8455749ef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_input = \"who is Ada Lovelace?\"\n",
    "#my_input = \"how do I contact Ada Lovelace?\"  #50/50\n",
    "#my_input = \"what is Ada Lovelace's phone number?\"\n",
    "#my_input = \"Wer ist Ada Lovlac?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd5c9b-c026-4e16-9708-48dcea51860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f2522-4f7b-4d2f-8d1b-9192b1f00f04",
   "metadata": {},
   "source": [
    "#### Experiment 2: retrieving information about a specific guest, not by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed0b95-09d2-42de-95a4-ba5bb184f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_input = \"what is the name of my inventor guest?\"\n",
    "#my_input = \"who of the people coming today is a physicist?\"\n",
    "#my_input = \"is the first programmer coming today? how do I know them?\"\n",
    "#my_input = \"what is the phone number of my gala guest who loves pigeons?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46540ffb-9942-4269-9d11-feea60657c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981948c-4bff-4010-890f-c5fc2bc4e3c2",
   "metadata": {},
   "source": [
    "#### Experiment 2.5 - asking collective information about the guests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028e6d2-3fc0-4695-a7ed-2037b9c4bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = \"how many people are coming today?\"\n",
    "#my_input = \"how many people are coming to the gala?\"\n",
    "#my_input = \"can you please tell me a fun fact about each guest at the gala?\"\n",
    "#my_input = \"can you tell me how women are guests of the gala?\"\n",
    "#my_input = \"there are no physicists coming to the gala, is that correct?\"\n",
    "#my_input = \"there are no physicists coming to the gala, is that correct? please check the attendees list and let me know\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abfddc-d369-4b50-9bff-bea9fd0827e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560902f-5949-48ca-9bda-a4f3d0d4948d",
   "metadata": {},
   "source": [
    "#### Experiment 3: mixing RAG context with general knowledge of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c925b85-a4bd-4276-ab11-794972e4bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_input = \"what would be a good gift for Nikola Tesla?\" #50/50\n",
    "#my_input = \"what would be a good gift for Nikola Tesla based on what you know about him from the gala guest information?\"\n",
    "#my_input = \"what is my guest Nikola Tesla's phone number? What would be a good pigeon breed to gift him?\"\n",
    "#my_input = \"What are the best pigeon breeds to give as a gift to Nikola Tesla? what is his phone number?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77e193-1efc-41a0-8921-7db0e8c9b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da31cb3-95ef-46e7-a67a-1e4e4298fc2e",
   "metadata": {},
   "source": [
    "#### Experiment 4: Adding more messages to Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adda60f-3b68-41c0-bfd3-7a299e33b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = \"what would be a good gift for Nikola Tesla?\" #50/50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97e592-0e6f-4863-8375-1f9cdf18c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d2ac1-3464-45aa-83e1-26a24ac83804",
   "metadata": {},
   "source": [
    "## Adding Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2275b-18e5-4cbe-bafe-f4a36fde04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99f4ff-7cca-4382-b8c4-4994f1f7b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = \"when is BVG next striking in Berlin?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868420e0-5347-4b63-afb5-09342e72acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try re-running this cell with the same query! \n",
    "results = search_tool.invoke(my_query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060c121-55b1-4bfa-942d-0e51823a2a8e",
   "metadata": {},
   "source": [
    "## Adding fake weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d3b00-0b4c-4da8-8cfc-d08d20982488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "import random\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = Tool(\n",
    "    name=\"get_weather_info\",\n",
    "    func=get_weather_info,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba89fa-7f15-484e-a1a2-a5f2cec98f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-Coder-Next\", #model in materials didn't work, newer one does\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [weather_info_tool\n",
    "        ,guest_info_tool\n",
    "        ]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f8a1a-ec5e-44e4-b4ce-f4384dcf9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try re-running these:\n",
    "#my_input = \"what is the weather like in Paris today?\"\n",
    "#my_input = \"what is the weather like in Marie Curie's hometown today?\"\n",
    "#my_input = \"what is the email address of Nikola Tesla?\"\n",
    "#my_input = \"what is the email address of my gala guest Nikola Tesla?\"\n",
    "#my_input = \"what is the email address of Nikola Tesla? what is the weather in his hometown? what is his home town\"\n",
    "#my_input = \"is today's weather suitable for pigeon racing? would any of my guests be interested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ee2e7-ac92-42bb-8429-78f380e74feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ae690-d0d8-4b55-b409-75700316fe42",
   "metadata": {},
   "source": [
    "## Adding HuggingFace Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6654e0f-cc08-4f40-8113-d27997218e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cd142-35f0-4b01-a784-b1bdb3150b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "my_input = \"facebook\"\n",
    "#my_input = \"Facebook\"\n",
    "print(hub_stats_tool.invoke(my_input)) # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9432b-65c9-4044-bd98-63317a59b8d4",
   "metadata": {},
   "source": [
    "**To do:** figure out a way to implement typos / case insensitive model search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e02ca-2c67-48e1-9f3f-7c44860010e0",
   "metadata": {},
   "source": [
    "### Combining all the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c441cb9-e369-4572-b21d-0cfb8e47cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen3-Coder-Next\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool,\n",
    "        search_tool,\n",
    "         weather_info_tool,\n",
    "         hub_stats_tool\n",
    "        ]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f94c90-78e6-49a0-b716-87b58b66893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = \"Who is Facebook and what's their most popular model on huggingface?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2e167-c454-47ac-afe5-3b248eb76039",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e11509-4774-474e-8263-1d6ee6fc24ba",
   "metadata": {},
   "source": [
    "#### Experiment 1: Combining RAG and other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f63d31-118e-4437-a497-f6ac87e03471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment guest info and re-initialise alfred for this - all tools on\n",
    "#my_input = \"Who is Facebook and what's their most popular model on huggingface?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7870fd8-41a5-4969-be7d-7b66ef1379e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with and without web search\n",
    "#my_input = \"what is the contact information for Marie Curie?\"\n",
    "#my_input = \"what is the contact information for my gala guest Marie Curie based on your guest information?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1510229-bced-476c-94d6-63fb9222a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with and without web search\n",
    "#my_input = \"what is Nikola Tesla's phone number and how do I know him?\"\n",
    "#my_input = \"what is my gala guest's Nikola Tesla's phone number and how do I know him?\"\n",
    "#my_input = \"where was Nikola Tesla born? what is his phone number and how do I know him?\"\n",
    "#my_input = \"where was Nikola Tesla, my gala guest, born? what is his phone number and how do I know him?\"\n",
    "#my_input = \"when is the next solar eclipse? where was Nikola Tesla born? how can I contact him?\"\n",
    "#my_input = \"when is the next solar eclipse? what is the weather in Nikola Tesla's hometown? how can I contact Marie Curie?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11c199-cab5-4550-bb73-2293d860a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=my_input)]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55caaee-6fed-4069-8d95-de85d2630344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a00fdf6f-c0af-40b0-a3c9-be7d56b148da",
   "metadata": {},
   "source": [
    "## Some open questions + experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eeb733-f4c1-4f5e-8eac-f23fe6f69036",
   "metadata": {},
   "source": [
    "### Does using embeddings change anything? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da20d12-7bc9-4efe-9515-4ace852cafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.invoke(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool_embeddings = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d84a88-76ac-4ffc-a39c-3a6078fc308e",
   "metadata": {},
   "source": [
    "### Can we make Alfred prioritise RAG results over internet search? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfceeaf-67c4-4b66-9eff-a0a9acbd89fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a6b23-f205-4b1d-8b68-0edbf808a2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_rag",
   "language": "python",
   "name": "agentic_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
